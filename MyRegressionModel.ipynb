{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own library to implement machine learning\n",
    "(Regression and Logistic Regression models for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd;\n",
    "from matplotlib import pyplot as plt;\n",
    "import math;\n",
    "#I generally add semicolons even in python. \n",
    "\n",
    "#designed for logistic Regression currently\n",
    "class LogisticRegression:\n",
    "    globalW = np.zeros(1);\n",
    "    criteria = 0.5; \n",
    "    globalB = 0;\n",
    "    features = 0; #(n )\n",
    "    MeanOfFeaturesTaken = np.zeros(features); \n",
    "    SDofFeaturesTaken = np.zeros(features); \n",
    "    Normalized = False;  #a bool to store the method of regression used\n",
    "    def __init__(self) -> None:\n",
    "        self.globalW = np.zeros(1);\n",
    "        self.globalB = 0;\n",
    "        pass\n",
    "    '''returns the sigmoid function for all elements of z(if z is a numpy array).\n",
    "    useful for classification problems using logistic regression'''\n",
    "    def sigmoid(z): \n",
    "        g = 1/(1 + np.exp(-z)); \n",
    "        return g;\n",
    "\n",
    "    '''uses numpy's vectorization to effeciently carry out finding the total cost'''\n",
    "    def FindTotalCostLogistic(X, y, w, b, regularizationFactor = 0):\n",
    "        \"\"\"\n",
    "        Computes the cost over all examples\n",
    "        Arguments as follows\n",
    "        X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "        y : (array_like Shape (m,)) target value \n",
    "        w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "        b : scalar Values of bias parameter of the model\n",
    "        regularizationFactor: used for regularization with higher order terms to prevent overfitting\n",
    "        Returns:\n",
    "        total_cost: (scalar)         cost \n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        m, n = X.shape; \n",
    "        newX = X.transpose(); \n",
    "\n",
    "        z_wb = np.dot(w,newX) + b; #Now z_wb is an m sized array\n",
    "        f_wb = LogisticRegression.sigmoid(z_wb);    #f2_wb = np.log(1-f_wb); f1_wb = np.log(f_wb);\n",
    "        f2_wb= np.log(1-f_wb); f1_wb = np.log(f_wb);\n",
    "        #print(z_wb); \n",
    "        # for i in range(m):\n",
    "        #     if(f_wb[i] == 1):\n",
    "        #         f2_wb[i] = -1; \n",
    "        #     else:\n",
    "        #         f2_wb[i] = np.log(1-f_wb[i]); \n",
    "\n",
    "        #f_wb[np.isnan(f_wb)] = 0; f2_wb[np.isnan(f2_wb)] = 0; f1_wb[np.isnan(f2_wb)] = 0; \n",
    "        loss = -y*f1_wb - (1-y)*f2_wb; \n",
    "        #important fact ist that this number must be positive always\n",
    "        #print(loss); \n",
    "        #print(np.sum(loss)); \n",
    "        regularizedCost = (regularizationFactor/(2 * m))*np.dot(w,w); \n",
    "        total_cost = np.sum(loss)/m + regularizedCost; \n",
    "        return total_cost; \n",
    "   \n",
    "    '''Computes the gradient for the input data'''\n",
    "    def CalculateGradientLogistic(X, y, w, b, regularizationFactor=0): \n",
    "        \"\"\"\n",
    "        Computes the gradient for logistic regression \n",
    "    \n",
    "        Args:\n",
    "        X : (ndarray Shape (m,n)) variable such as house size \n",
    "        y : (array_like Shape (m,1)) actual value \n",
    "        w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "        b : (scalar)                 value of parameter of the model \n",
    "        regularizationFactor: for regularization. 0 for no regularization.\n",
    "        Returns\n",
    "        dJdW: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "        dJdB: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "        \"\"\"\n",
    "        m, n = X.shape; \n",
    "        dJdW = np.zeros(w.shape); \n",
    "        T = X.transpose(); \n",
    "        allZ = np.dot(w,T) + b; #vectorized\n",
    "        allF = LogisticRegression.sigmoid(allZ) - y; #vectorized\n",
    "        dJdB = np.sum(allF)/m; #vectorized\n",
    "        \n",
    "        modifiedAllF = allF*T; #vectorized\n",
    "        for j in range(n):\n",
    "            dJdW[j] = np.sum(modifiedAllF[j])/m; #vectorized over the examples, but not over the features      \n",
    "        \n",
    "        regularizedAddition = w*regularizationFactor/m; \n",
    "        dJdW += regularizedAddition; #to implement regularization of the w coeffecients\n",
    "        return dJdB, dJdW; \n",
    "    \n",
    "    '''performs gradient descent to learn and improve the variables'''\n",
    "    def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, regularizationFactor): \n",
    "        \"\"\"\n",
    "        Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "        num_iters gradient steps with learning rate alpha\n",
    "        \n",
    "        Args:\n",
    "        X :    (array_like Shape (m, n)\n",
    "        y :    (array_like Shape (m,))\n",
    "        w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "        b_in : (scalar)                 Initial value of parameter of the model\n",
    "        cost_function:                  function to compute cost\n",
    "        alpha : (float)                 Learning rate\n",
    "        num_iters : (int)               number of iterations to run gradient descent\n",
    "        regularizationFactor (scalar, float)         regularization constant\n",
    "        \n",
    "        Returns:\n",
    "        w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "            running gradient descent\n",
    "        b : (scalar)                Updated value of parameter of the model after\n",
    "            running gradient descent\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of training examples\n",
    "        m = len(X); \n",
    "        \n",
    "        # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "        J_history = []; \n",
    "        w_history = []; \n",
    "        \n",
    "        for i in range(num_iters):\n",
    "\n",
    "            # Calculate the gradient and update the parameters\n",
    "            dJdB, dJdW = gradient_function(X, y, w_in, b_in, regularizationFactor); \n",
    "            # Update Parameters using w, b, alpha and gradient\n",
    "            w_in = w_in - alpha * dJdW;               \n",
    "            b_in = b_in - alpha * dJdB;             \n",
    "        \n",
    "            # Save cost J at each iteration for checking later\n",
    "            if i<100000:      # prevent resource exhaustion \n",
    "                cost =  cost_function(X, y, w_in, b_in, regularizationFactor); \n",
    "                J_history.append(cost); \n",
    "\n",
    "            # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "            if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "                w_history.append(w_in); \n",
    "                print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.6f}   \"); \n",
    "            \n",
    "        return w_in, b_in, J_history, w_history; #return w and J,w history for graphing\n",
    "    \n",
    "\n",
    "    '''gives the final output 0 or 1 using the learned parameters\n",
    "    returns a list of 0's and 1's that depict whether the ith input was predicted as 0 or 1'''\n",
    "    def predict(self, X, w, b):\n",
    "        if(w == None):\n",
    "            w = self.globalW; \n",
    "        if(b == None):\n",
    "            b = self.globalB;  \n",
    "        \"\"\"        \n",
    "        Args:\n",
    "        X : (ndarray Shape (m, n))\n",
    "        w : (array_like Shape (n,))      Parameters of the model\n",
    "        b : (scalar, float)              Parameter of the model\n",
    "        Returns:\n",
    "        p: (ndarray (m,1))\n",
    "            The predictions for X using a threshold at 0.5\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        m, n = X.shape   \n",
    "        p = np.zeros(m)\n",
    "        #important, we should probably be trying to normalize everything first\n",
    "        \n",
    "        T = X.transpose();\n",
    "        print(\"here is the SD\")\n",
    "        print(self.SDofFeaturesTaken); \n",
    "        #then convert each value\n",
    "        for i in range(n):\n",
    "            T[i] = (T[i] - self.MeanOfFeaturesTaken[i])/self.SDofFeaturesTaken[i]; \n",
    "        print(T[:5]); \n",
    "        allZ = np.dot(w,T) + b;\n",
    "        allF = LogisticRegression.sigmoid(allZ);\n",
    "        for i in range(0,m):\n",
    "            if(allF[i] >= self.criteria):\n",
    "                p[i] = 1;\n",
    "        return p\n",
    "\n",
    "\n",
    "    def predictNormalized(self, X, w, b):\n",
    "        if(w == None):\n",
    "            w = self.globalW;\n",
    "        if(b == None):\n",
    "            b = self.globalB;  \n",
    "        # number of training examples\n",
    "        m, n = X.shape   \n",
    "        p = np.zeros(m)\n",
    "        #important, we should probably be trying to normalize everything first\n",
    "        \n",
    "        T = X.transpose();\n",
    "        #then convert each value\n",
    "        for i in range(n):\n",
    "            T[i] = (T[i] - self.MeanOfFeaturesTaken[i])/self.SDofFeaturesTaken[i]; \n",
    "        \n",
    "        #after this everything should be normalized\n",
    "        allZ = np.dot(w,T) + b;\n",
    "        allF = LogisticRegression.sigmoid(allZ);\n",
    "        for i in range(0,m):\n",
    "            if(allF[i] >= self.criteria):\n",
    "                p[i] = 1;\n",
    "        return p\n",
    "    def UnnormalizedRegressionTrain(self,x_train,y_train,alpha = 0.01,iterations=1000,regularizationFactor=0):\n",
    "        #here x_train is an mxn array of numbers\n",
    "        #and y_train is a list of 0's and 1's  corresponding to the outcome\n",
    "        #first we calculate the values of w and \n",
    "        m,n = x_train.shape; \n",
    "        if(self.globalW.shape != (n,)):\n",
    "            self.globalW = np.zeros(n); \n",
    "        \n",
    "        # self.MeanOfFeaturesTaken = np.zeros(n); \n",
    "        # self.SDofFeaturesTaken = np.ones(n);  \n",
    "        self.globalW,self.globalB,CostHistory,WHistory = LogisticRegression.gradient_descent(x_train,y_train,self.globalW,self.globalB,LogisticRegression.FindTotalCostLogistic,\n",
    "        LogisticRegression.CalculateGradientLogistic,alpha,iterations,regularizationFactor);\n",
    "\n",
    "        return CostHistory, WHistory;  \n",
    "        #now that we have these parameters, we can call the predict function directly\n",
    "    \n",
    "    '''used for implementing normalization on the data so the ranges of each feature become comparable to each other\n",
    "    results in faster gradient descent'''\n",
    "    def NormalizedRegressionTrain(self, x_train, y_train, alpha = 0.01, iterations = 1000, regularizationFactor = 0):\n",
    "        #first we need to convert each of xtrain inputs to (x - mu)/ sd where mu is average and sd is standard deviation\n",
    "        #we achieve this through a simple loop\n",
    "        self.Normalized = True; \n",
    "        m,n = x_train.shape; \n",
    "        self.MeanOfFeaturesTaken = np.zeros(n); \n",
    "        self.SDofFeaturesTaken = np.zeros(n);  \n",
    "        T = x_train.transpose(); \n",
    "        for i in range(n):\n",
    "            self.MeanOfFeaturesTaken[i] = np.sum(T[i])/m; \n",
    "        #after the mean has been processed, we can also find the standard deviation similarly\n",
    "        #DisplacedT = T - self.MeanOfFeaturesTaken; \n",
    "        DisplacedT = np.zeros(n*m).reshape(n,m); \n",
    "        for i in range(n):\n",
    "            #self.SDofFeaturesTaken[i] = math.sqrt(np.dot(T[i]-self.MeanOfFeaturesTaken[i],T[i]-self.MeanOfFeaturesTaken[i])/m); \n",
    "            DisplacedT[i] = T[i] - self.MeanOfFeaturesTaken[i];\n",
    "            self.SDofFeaturesTaken[i] = math.sqrt(np.dot(DisplacedT[i],DisplacedT[i])/m); \n",
    "         \n",
    "        #now the average and SD has been computed\n",
    "        #take note that SD must not be 0, and hence no input should be constant for using this version of regression Training.\n",
    "        #now transform the inputs before training\n",
    "        for i in range(n):\n",
    "            DisplacedT[i] /= self.SDofFeaturesTaken[i]; \n",
    "        X = DisplacedT.transpose(); \n",
    "        # print(self.MeanOfFeaturesTaken);   \n",
    "        # print(self.SDofFeaturesTaken); \n",
    "        \n",
    "        return self.UnnormalizedRegressionTrain(X,y_train,alpha,iterations,regularizationFactor); \n",
    "    \n",
    "    '''converts features to higher degrees, regularization required to prevent overfitting'''\n",
    "    def FeatureExtraDegree(self, X_train):\n",
    "        #should form n*(n+1)/2 total features at the end\n",
    "        m,n = X_train.shape; \n",
    "        final = np.zeros(m*(int(n*(n+1)/2) + n)).reshape((int(n*(n+1)/2) + n),m); \n",
    "        T = X_train.transpose(); \n",
    "        pos = 0; \n",
    "        for i in range(n):\n",
    "            for j in range(i,n):\n",
    "                #multiply the two terms together to get a new factor\n",
    "                final[pos] = T[i]*T[j]; \n",
    "                pos+=1;\n",
    "        for j in range(n):\n",
    "            final[pos] = T[i]; \n",
    "            pos+=1; \n",
    "        return (final.transpose());  \n",
    "\n",
    "    def FeatureHigherDegree(X_train, degree):\n",
    "        #will be used to make higher degree terms of the feature combinations\n",
    "        #can be done directly\n",
    "        \n",
    "        pass;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on some short data:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.692861   \n",
      "Iteration 1000: Cost 0.503382   \n",
      "Iteration 2000: Cost 0.405594   \n",
      "Iteration 3000: Cost 0.341363   \n",
      "Iteration 4000: Cost 0.294354   \n",
      "Iteration 5000: Cost 0.258092   \n",
      "Iteration 6000: Cost 0.229231   \n",
      "Iteration 7000: Cost 0.205748   \n",
      "Iteration 8000: Cost 0.186309   \n",
      "Iteration 9000: Cost 0.169988   \n",
      "Iteration 9999: Cost 0.156129   \n",
      "here is the SD\n",
      "[1.0198039  1.49666295 2.72763634 0.74833148 3.26190129 3.72021505\n",
      " 3.49857114]\n",
      "[[-1  0  1  0  0]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0 -1  1]\n",
      " [ 0 -1  0  1  1]\n",
      " [ 0 -1  1  0  1]]\n",
      "[1. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([[2,3,5,4,3],[3,5,3,4,7],[4,6,3,1,9],[4,3,4,5,5],[2,1,9,3,8],[4,1,11,6,10],[10,4,9,12,3]]); \n",
    "x_train = x_train.transpose(); \n",
    "y_train = np.array([1,0,0,0,1]); \n",
    "myLR = LogisticRegression(); \n",
    "myLR.NormalizedRegressionTrain(x_train,y_train,iterations=10000,alpha=0.001);\n",
    "\n",
    "print(myLR.predict(x_train,None,None)); \n",
    "\n",
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the Titanic Disaster Machine Learning competition on Kaggle,\n",
    "Initially with only linear parameters of just 7 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "PassengerId                                                    \n",
       "1               3.0  0.0  22.0    1.0    0.0   7.2500       0.0\n",
       "2               1.0  1.0  38.0    1.0    0.0  71.2833       1.0\n",
       "3               3.0  1.0  26.0    0.0    0.0   7.9250       0.0\n",
       "4               1.0  1.0  35.0    1.0    0.0  53.1000       0.0\n",
       "5               3.0  0.0  35.0    0.0    0.0   8.0500       0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/TitanicTraining.csv\",index_col=0) \n",
    "#m,n = train_data.size; \n",
    "train_data.head() \n",
    "usefulColumns = [1,3,4,5,6,8,10];\n",
    "y_train = train_data.loc[:,'Survived']; \n",
    "train_data = train_data.iloc[:,usefulColumns]\n",
    "m,n = train_data.shape\n",
    "for i in range(1,m+1):\n",
    "    c = train_data.loc[i,'Embarked'];\n",
    "    if(c == 'S'):\n",
    "        train_data.loc[i,'Embarked'] = 0; \n",
    "    elif(c == 'Q'):\n",
    "        train_data.loc[i,'Embarked'] = -1; \n",
    "    else:\n",
    "        train_data.loc[i,'Embarked'] = 1;\n",
    "    if(train_data.loc[i,'Sex'] == 'male'):\n",
    "        train_data.loc[i,'Sex'] = 0; \n",
    "    else:\n",
    "        train_data.loc[i,'Sex'] = 1; \n",
    "train_data.Age =  train_data.Age.fillna(float(30))\n",
    "\n",
    "\n",
    "\n",
    "#now we have effectively reduced the number of features to input to our model\n",
    "#Survived should be a y_train for this\n",
    "#the features I will use to test my ML library are Pclass, Sex, Age, SibSp, Parch, Fare and Embarked\n",
    "train_data.Sex = train_data.Sex.astype('float64');\n",
    "train_data.Age = train_data.Age.astype('float64');\n",
    "train_data.Fare = train_data.Fare.astype('float64');\n",
    "train_data.Embarked = train_data.Embarked.astype('float64');\n",
    "train_data = train_data.astype('float64');\n",
    "train_data.to_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/myTrainingDataOnlyFirst.csv\")\n",
    "train_data.head()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n",
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "newTrainData = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/myTrainingDataOnlyFirst.csv\"); \n",
    "newTrainData = newTrainData.drop('PassengerId',axis = 'columns')\n",
    "arrayXdata = newTrainData.to_numpy();\n",
    "arrayYdata =  y_train.to_numpy();  \n",
    "print(arrayXdata.shape);\n",
    "print(arrayYdata.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.693134   \n",
      "Iteration 10000: Cost 0.597264   \n",
      "Iteration 20000: Cost 0.546063   \n",
      "Iteration 30000: Cost 0.515764   \n",
      "Iteration 40000: Cost 0.496411   \n",
      "Iteration 50000: Cost 0.483347   \n",
      "Iteration 60000: Cost 0.474155   \n",
      "Iteration 70000: Cost 0.467476   \n",
      "Iteration 80000: Cost 0.462499   \n",
      "Iteration 90000: Cost 0.458712   \n",
      "Iteration 99999: Cost 0.455779   \n"
     ]
    }
   ],
   "source": [
    "#starting the machine learning algorithm\n",
    "TitanicLR = LogisticRegression(); \n",
    "#TitanicLR.UnnormalizedRegressionTrain(arrayXdata,arrayYdata,0.001,100000,20);\n",
    "TitanicLR.Normalized = True; \n",
    "TitanicLR.criteria = 0.5; \n",
    "AddedDegrees = TitanicLR.FeatureExtraDegree(arrayXdata); \n",
    "TitanicLR.NormalizedRegressionTrain(arrayXdata,arrayYdata,0.0001,100000,0); \n",
    "\n",
    "#then we test the prediction given on the testcase we have\n",
    "\n",
    "\n",
    "#print(AddedDegrees.shape); \n",
    "# print(TitanicLR.Normalized); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the SD\n",
      "[ 0.83560193  0.47772176 12.99527138  1.10212444  0.80560476 49.66553444\n",
      "  0.51606398]\n",
      "[[-13.06452586 -17.97397556 -13.06452586 ... -13.06452586 -17.97397556\n",
      "  -13.06452586]\n",
      " [-26.44431454  13.74632038  13.74632038 ...  13.74632038 -26.44431454\n",
      "  -26.44431454]\n",
      " [ -2.48081895  -2.48077578  -2.48080816 ...  -2.48079737  -2.48080816\n",
      "   -2.48079197]\n",
      " [ -1.35693797  -1.35693797  -1.97189794 ...  -1.35693797  -1.97189794\n",
      "   -1.97189794]\n",
      " [ -3.82204932  -3.82204932  -3.82204932 ...   2.0720829   -3.82204932\n",
      "   -3.82204932]]\n",
      "% correct = 78.67564534231201\n",
      "predicted deaths = 577\n",
      "actual deaths= 549\n",
      "actually died but predicted lived  81\n",
      "actually lived but predicted died  109\n",
      "[[-13.06452586 -17.97397556 -13.06452586 ... -13.06452586 -17.97397556\n",
      "  -13.06452586]\n",
      " [-26.44431454  13.74632038  13.74632038 ...  13.74632038 -26.44431454\n",
      "  -26.44431454]\n",
      " [ -2.48081895  -2.48077578  -2.48080816 ...  -2.48079737  -2.48080816\n",
      "   -2.48079197]\n",
      " [ -1.35693797  -1.35693797  -1.97189794 ...  -1.35693797  -1.97189794\n",
      "   -1.97189794]\n",
      " [ -3.82204932  -3.82204932  -3.82204932 ...   2.0720829   -3.82204932\n",
      "   -3.82204932]]\n"
     ]
    }
   ],
   "source": [
    "#testing the prediction rate of our algorithm on the training data\n",
    "predictedOutputTrain = TitanicLR.predict(arrayXdata,None,None); \n",
    "matchings = 0; \n",
    "actualDeaths = 0; predictedDeaths = 0; \n",
    "ActualDeathToLife = 0; actualLifeToDeath = 0;\n",
    "for i in range(len(predictedOutputTrain)):\n",
    "    if(predictedOutputTrain[i] == arrayYdata[i]):\n",
    "        matchings += 1; \n",
    "    else:\n",
    "        if(arrayYdata[i] == 1):\n",
    "            actualLifeToDeath+=1;\n",
    "        else:\n",
    "            ActualDeathToLife += 1;\n",
    "        \n",
    "    if(predictedOutputTrain[i] == 0):\n",
    "        predictedDeaths+=1;\n",
    "    if(arrayYdata[i] == 0):\n",
    "        actualDeaths += 1; \n",
    "\n",
    "print(\"% correct =\" ,matchings*100/len(predictedOutputTrain)); \n",
    "print(\"predicted deaths =\" , predictedDeaths); print(\"actual deaths=\", actualDeaths); \n",
    "print(\"actually died but predicted lived \", ActualDeathToLife);\n",
    "print(\"actually lived but predicted died \", actualLifeToDeath);\n",
    "print(arrayXdata.transpose()[:5]); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the code above ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 7)\n",
      "True\n",
      "(418, 7)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#then we remove the Name column and passenger ID as those are unnecessary\n",
    "#and convert all the string inputs to numerical inputs\n",
    "test_data = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/test.csv\",index_col=0);\n",
    "\n",
    "test_data = test_data.drop('Name',axis = 'columns'); \n",
    "test_data = test_data.drop('Ticket',axis = 'columns'); \n",
    "test_data = test_data.drop('Cabin',axis = 'columns'); \n",
    "m,n = test_data.shape\n",
    "test_data.Age =  test_data.Age.fillna(float(30))\n",
    "#print(test_data.shape)\n",
    "#test_data.loc['Embarked']; \n",
    "# for i in range(896,891+m+1):\n",
    "#     c = train_data.loc[i,'Embarked'];\n",
    "    # if(c == 'S'):\n",
    "    #     train_data.loc[i,'Embarked'] = 0; \n",
    "    # elif(c == 'Q'):\n",
    "    #     train_data.loc[i,'Embarked'] = -1; \n",
    "    # else:\n",
    "    #     train_data.loc[i,'Embarked'] = 1;\n",
    "#     if(train_data.loc[i,'Sex'] == 'male'):\n",
    "#         train_data.loc[i,'Sex'] = 0; \n",
    "#     else:\n",
    "#         train_data.loc[i,'Sex'] = 1; \n",
    "for i in range(892,1310):\n",
    "    c = test_data.loc[i,'Embarked']; \n",
    "    if(c == 'S'):\n",
    "        test_data.loc[i,'Embarked'] = 0; \n",
    "    elif(c == 'Q'):\n",
    "        test_data.loc[i,'Embarked'] = -1; \n",
    "    else:\n",
    "        test_data.loc[i,'Embarked'] = 1;\n",
    "    if(test_data.loc[i,'Sex'] == 'male'):\n",
    "        test_data.loc[i,'Sex'] = 0; \n",
    "    else:\n",
    "        test_data.loc[i,'Sex'] = 1; \n",
    "test_data.Sex = test_data.Sex.astype('float64');\n",
    "test_data.Age = test_data.Age.astype('float64');\n",
    "test_data.Fare = test_data.Fare.astype('float64');\n",
    "test_data.Embarked = test_data.Embarked.astype('float64');\n",
    "test_data = test_data.astype('float64');\n",
    "test_data.to_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/EditedTestingFile2.csv\", index=False)\n",
    "test_data = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/EditedTestingFile2.csv\", index_col=False)\n",
    "print(test_data.shape); \n",
    "testingXdata = test_data.to_numpy(); \n",
    "print(TitanicLR.Normalized); \n",
    "#testingXdata = TitanicLR.FeatureExtraDegree(testingXdata);  #use when second degree features are considered\n",
    "print(testingXdata.shape); \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is the SD\n",
      "[ 0.83560193  0.47772176 12.99527138  1.10212444  0.80560476 49.66553444\n",
      "  0.51606398]\n",
      "[[ 0.82737724  0.82737724 -0.36936484 ...  0.82737724  0.82737724\n",
      "   0.82737724]\n",
      " [-0.73769513  1.35557354 -0.73769513 ... -0.73769513 -0.73769513\n",
      "  -0.73769513]\n",
      " [ 0.36483356  1.3267219   2.48098791 ...  0.67263783  0.01855376\n",
      "   0.01855376]\n",
      " [-0.4745452   0.43279337 -0.4745452  ... -0.4745452  -0.4745452\n",
      "   0.43279337]\n",
      " [-0.47367361 -0.47367361 -0.47367361 ... -0.47367361 -0.47367361\n",
      "   0.76762988]]\n"
     ]
    }
   ],
   "source": [
    "outputY = TitanicLR.predict(testingXdata,None,None); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53805332  1.00468557 -0.25966058 -0.20307676 -0.03268501  0.24328424\n",
      "  0.10012872]\n",
      "-0.47084978609827316\n"
     ]
    }
   ],
   "source": [
    "print(TitanicLR.globalW); \n",
    "print(TitanicLR.globalB); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the output received back to a dataframe and then to a csv to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [i for i in range(892,1310)]; \n",
    "#print(indices); \n",
    "finalOutputDataFrame = pd.DataFrame({'PassengerId': indices ,'Survived': outputY}); \n",
    "\n",
    "# print(outputY.size); \n",
    "# print(len(indices))\n",
    "# print(testingXdata.shape)\n",
    "finalOutputDataFrame.Survived = finalOutputDataFrame.Survived.astype('Int64'); \n",
    "finalOutputDataFrame.PassengerId = finalOutputDataFrame.PassengerId.astype('Int64'); \n",
    "finalOutputDataFrame.to_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/TitanicPredictions/AnswerFirstDegree4.csv\", index=False)\n",
    "finalOutputDataFrame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdc9137cb471303d8b237a9ab8db7e09d2a92a20f8ca4516f32bde7bacd44699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
