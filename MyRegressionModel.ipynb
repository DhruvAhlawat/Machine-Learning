{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My own library to implement machine learning\n",
    "(Regression and Logistic Regression models for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd;\n",
    "from matplotlib import pyplot as plt;\n",
    "import math;\n",
    "#I generally add semicolons even in python. \n",
    "\n",
    "#designed for logistic Regression currently\n",
    "class LogisticRegression:\n",
    "    globalW = 0;\n",
    "    globalB = 0;\n",
    "    def __init__(self) -> None:\n",
    "        self.globalW = 0;\n",
    "        self.globalB = 0;\n",
    "        pass\n",
    "    '''returns the sigmoid function for all elements of z(if z is a numpy array).\n",
    "    useful for classification problems using logistic regression'''\n",
    "    def sigmoid(z): \n",
    "        g = 1/(1 + np.exp(-z)); \n",
    "        return g;\n",
    "\n",
    "    '''uses numpy's vectorization to effeciently carry out finding the total cost'''\n",
    "    def FindTotalCostLogistic(X, y, w, b, regularizationFactor = 0):\n",
    "        \"\"\"\n",
    "        Computes the cost over all examples\n",
    "        Arguments as follows\n",
    "        X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "        y : (array_like Shape (m,)) target value \n",
    "        w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "        b : scalar Values of bias parameter of the model\n",
    "        regularizationFactor: used for regularization with higher order terms to prevent overfitting\n",
    "        Returns:\n",
    "        total_cost: (scalar)         cost \n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        m, n = X.shape; \n",
    "        newX = X.transpose(); \n",
    "\n",
    "        z_wb = np.dot(w,newX) + b; #Now z_wb is an m sized array\n",
    "        f_wb = LogisticRegression.sigmoid(z_wb);    #f2_wb = np.log(1-f_wb); f1_wb = np.log(f_wb);\n",
    "        f2_wb= np.log(1-f_wb); f1_wb = np.log(f_wb);\n",
    "        #print(z_wb); \n",
    "\n",
    "        # for i in range(m):\n",
    "        #     if(f_wb[i] == 1):\n",
    "        #         f2_wb[i] = -1; \n",
    "        #     else:\n",
    "        #         f2_wb[i] = np.log(1-f_wb[i]); \n",
    "\n",
    "        #f_wb[np.isnan(f_wb)] = 0; f2_wb[np.isnan(f2_wb)] = 0; f1_wb[np.isnan(f2_wb)] = 0; \n",
    "        loss = -y*f1_wb - (1-y)*f2_wb; \n",
    "        #important fact ist that this number must be positive always\n",
    "        #print(loss); \n",
    "        #print(np.sum(loss)); \n",
    "        regularizedCost = (regularizationFactor/(2 * m))*np.dot(w,w); \n",
    "        total_cost = np.sum(loss)/m + regularizedCost; \n",
    "        return total_cost; \n",
    "   \n",
    "    '''Computes the gradient for the input data'''\n",
    "    def CalculateGradientLogistic(X, y, w, b, regularizationFactor=0): \n",
    "        \"\"\"\n",
    "        Computes the gradient for logistic regression \n",
    "    \n",
    "        Args:\n",
    "        X : (ndarray Shape (m,n)) variable such as house size \n",
    "        y : (array_like Shape (m,1)) actual value \n",
    "        w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "        b : (scalar)                 value of parameter of the model \n",
    "        regularizationFactor: for regularization. 0 for no regularization.\n",
    "        Returns\n",
    "        dJdW: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "        dJdB: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "        \"\"\"\n",
    "        m, n = X.shape; \n",
    "        dJdW = np.zeros(w.shape); \n",
    "        T = X.transpose(); \n",
    "        allZ = np.dot(w,T) + b; #vectorized\n",
    "        allF = LogisticRegression.sigmoid(allZ) - y; #vectorized\n",
    "        dJdB = np.sum(allF)/m; #vectorized\n",
    "        \n",
    "        modifiedAllF = allF*T; #vectorized\n",
    "        for j in range(n):\n",
    "            dJdW[j] = np.sum(modifiedAllF[j])/m; #vectorized over the examples, but not over the features      \n",
    "        \n",
    "        regularizedAddition = w*regularizationFactor/m; \n",
    "        dJdW += regularizedAddition; #to implement regularization of the w coeffecients\n",
    "        return dJdB, dJdW; \n",
    "    \n",
    "    '''performs gradient descent to learn and improve the variables'''\n",
    "    def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, regularizationFactor): \n",
    "        \"\"\"\n",
    "        Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "        num_iters gradient steps with learning rate alpha\n",
    "        \n",
    "        Args:\n",
    "        X :    (array_like Shape (m, n)\n",
    "        y :    (array_like Shape (m,))\n",
    "        w_in : (array_like Shape (n,))  Initial values of parameters of the model\n",
    "        b_in : (scalar)                 Initial value of parameter of the model\n",
    "        cost_function:                  function to compute cost\n",
    "        alpha : (float)                 Learning rate\n",
    "        num_iters : (int)               number of iterations to run gradient descent\n",
    "        regularizationFactor (scalar, float)         regularization constant\n",
    "        \n",
    "        Returns:\n",
    "        w : (array_like Shape (n,)) Updated values of parameters of the model after\n",
    "            running gradient descent\n",
    "        b : (scalar)                Updated value of parameter of the model after\n",
    "            running gradient descent\n",
    "        \"\"\"\n",
    "        \n",
    "        # number of training examples\n",
    "        m = len(X); \n",
    "        \n",
    "        # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "        J_history = []; \n",
    "        w_history = []; \n",
    "        \n",
    "        for i in range(num_iters):\n",
    "\n",
    "            # Calculate the gradient and update the parameters\n",
    "            dJdB, dJdW = gradient_function(X, y, w_in, b_in, regularizationFactor); \n",
    "            # Update Parameters using w, b, alpha and gradient\n",
    "            w_in = w_in - alpha * dJdW;               \n",
    "            b_in = b_in - alpha * dJdB;             \n",
    "        \n",
    "            # Save cost J at each iteration for checking later\n",
    "            if i<100000:      # prevent resource exhaustion \n",
    "                cost =  cost_function(X, y, w_in, b_in, regularizationFactor); \n",
    "                J_history.append(cost); \n",
    "\n",
    "            # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "            if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "                w_history.append(w_in); \n",
    "                print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \"); \n",
    "            \n",
    "        return w_in, b_in, J_history, w_history; #return w and J,w history for graphing\n",
    "    \n",
    "    # UNQ_C4\n",
    "# GRADED FUNCTION: predict\n",
    "    '''gives the final output 0 or 1 using the learned parameters\n",
    "    returns a list of 0's and 1's that depict whether the ith input was predicted as 0 or 1'''\n",
    "    def predict(self, X, w, b):\n",
    "        if(w == None):\n",
    "            w = self.globalW; \n",
    "        if(b == None):\n",
    "            b = self.globalB;  \n",
    "        \"\"\"        \n",
    "        Args:\n",
    "        X : (ndarray Shape (m, n))\n",
    "        w : (array_like Shape (n,))      Parameters of the model\n",
    "        b : (scalar, float)              Parameter of the model\n",
    "        Returns:\n",
    "        p: (ndarray (m,1))\n",
    "            The predictions for X using a threshold at 0.5\n",
    "        \"\"\"\n",
    "        # number of training examples\n",
    "        m, n = X.shape   \n",
    "        p = np.zeros(m)\n",
    "        #important, we should probably be trying to normalize everything first\n",
    "        ### START CODE HERE ### \n",
    "        T = X.transpose();\n",
    "        allZ = np.dot(w,T) + b;\n",
    "        allF = LogisticRegression.sigmoid(allZ);\n",
    "        for i in range(0,m):\n",
    "            if(allF[i] >= 0.5):\n",
    "                p[i] = 1;\n",
    "        return p\n",
    "\n",
    "    def UnnormalizedRegressionTrain(self,x_train,y_train,alpha = 0.01,iterations=1000,regularizationFactor=0):\n",
    "        #here x_train is an mxn array of numbers\n",
    "        #and y_train is a list of 0's and 1's  corresponding to the outcome\n",
    "        #first we calculate the values of w and \n",
    "        m,n = x_train.shape; \n",
    "        w = np.zeros(n); b = 0; \n",
    "        self.globalW,self.globalB,CostHistory,WHistory = LogisticRegression.gradient_descent(x_train,y_train,w,b,LogisticRegression.FindTotalCostLogistic,\n",
    "        LogisticRegression.CalculateGradientLogistic,alpha,iterations,regularizationFactor);\n",
    "\n",
    "        return CostHistory, WHistory;  \n",
    "        #now that we have these parameters, we can call the predict function directly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on some short data:- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.69   \n",
      "Iteration 1000: Cost     0.44   \n",
      "Iteration 2000: Cost     0.34   \n",
      "Iteration 3000: Cost     0.27   \n",
      "Iteration 4000: Cost     0.22   \n",
      "Iteration 5000: Cost     0.19   \n",
      "Iteration 6000: Cost     0.16   \n",
      "Iteration 7000: Cost     0.14   \n",
      "Iteration 8000: Cost     0.12   \n",
      "Iteration 9000: Cost     0.11   \n",
      "Iteration 9999: Cost     0.10   \n",
      "[1. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array([[2,3,5,4,3],[3,5,3,4,7],[4,6,3,1,9],[4,3,4,5,5],[2,1,9,3,8],[4,1,11,6,10],[10,4,9,12,3]]); \n",
    "x_train = x_train.transpose(); \n",
    "y_train = np.array([1,0,0,0,1]); \n",
    "myLR = LogisticRegression(); \n",
    "myLR.UnnormalizedRegressionTrain(x_train,y_train,iterations=10000,alpha=0.001);\n",
    "\n",
    "print(myLR.predict(x_train,None,None)); \n",
    "\n",
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the Titanic Disaster Machine Learning competition on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "PassengerId                                                    \n",
       "1               3.0  0.0  22.0    1.0    0.0   7.2500       0.0\n",
       "2               1.0  1.0  38.0    1.0    0.0  71.2833       1.0\n",
       "3               3.0  1.0  26.0    0.0    0.0   7.9250       0.0\n",
       "4               1.0  1.0  35.0    1.0    0.0  53.1000       0.0\n",
       "5               3.0  0.0  35.0    0.0    0.0   8.0500       0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/TitanicTraining.csv\",index_col=0) \n",
    "#m,n = train_data.size; \n",
    "test_data = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/test.csv\",index_col=0);\n",
    "train_data.head() \n",
    "usefulColumns = [1,3,4,5,6,8,10];\n",
    "y_train = train_data.loc[:,'Survived']; \n",
    "train_data = train_data.iloc[:,usefulColumns]\n",
    "m,n = train_data.shape\n",
    "for i in range(1,m+1):\n",
    "    c = train_data.loc[i,'Embarked'];\n",
    "    if(c == 'S'):\n",
    "        train_data.loc[i,'Embarked'] = 0; \n",
    "    elif(c == 'Q'):\n",
    "        train_data.loc[i,'Embarked'] = -1; \n",
    "    else:\n",
    "        train_data.loc[i,'Embarked'] = 1;\n",
    "    if(train_data.loc[i,'Sex'] == 'male'):\n",
    "        train_data.loc[i,'Sex'] = 0; \n",
    "    else:\n",
    "        train_data.loc[i,'Sex'] = 1; \n",
    "train_data.Age =  train_data.Age.fillna(float(30))\n",
    "\n",
    "\n",
    "\n",
    "#now we have effectively reduced the number of features to input to our model\n",
    "#Survived should be a y_train for this\n",
    "#the features I will use to test my ML library are Pclass, Sex, Age, SibSp, Parch, Fare and Embarked\n",
    "train_data.Sex = train_data.Sex.astype('float64');\n",
    "train_data.Age = train_data.Age.astype('float64');\n",
    "train_data.Fare = train_data.Fare.astype('float64');\n",
    "train_data.Embarked = train_data.Embarked.astype('float64');\n",
    "train_data = train_data.astype('float64');\n",
    "train_data.to_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/myTrainingDataOnlyFirst.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 7)\n",
      "(891,)\n"
     ]
    }
   ],
   "source": [
    "newTrainData = pd.read_csv(\"/OneDrive - IIT Delhi/Pictures/_PythonProjects/Machine Learning/CSV/myTrainingDataOnlyFirst.csv\"); \n",
    "newTrainData = newTrainData.drop('PassengerId',axis = 'columns')\n",
    "arrayXdata = newTrainData.to_numpy();\n",
    "arrayYdata =  y_train.to_numpy();\n",
    "#arrayXdata = arrayXdata.transpose();   \n",
    "print(arrayXdata.shape);\n",
    "print(arrayYdata.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.67   \n",
      "Iteration 10000: Cost     0.52   \n",
      "Iteration 20000: Cost     0.48   \n",
      "Iteration 30000: Cost     0.46   \n",
      "Iteration 40000: Cost     0.46   \n",
      "Iteration 50000: Cost     0.45   \n",
      "Iteration 60000: Cost     0.45   \n",
      "Iteration 70000: Cost     0.45   \n",
      "Iteration 80000: Cost     0.45   \n",
      "Iteration 90000: Cost     0.45   \n",
      "Iteration 99999: Cost     0.45   \n"
     ]
    }
   ],
   "source": [
    "#starting the machine learning algorithm\n",
    "TitanicLR = LogisticRegression(); \n",
    "TitanicLR.UnnormalizedRegressionTrain(arrayXdata,arrayYdata,0.001,100000,0); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdc9137cb471303d8b237a9ab8db7e09d2a92a20f8ca4516f32bde7bacd44699"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
